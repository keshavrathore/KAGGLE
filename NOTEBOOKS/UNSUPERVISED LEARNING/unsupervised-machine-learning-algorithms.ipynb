{"cells":[{"metadata":{"_uuid":"9da2feee15a0eedea30241d07c079a89aa922932"},"cell_type":"markdown","source":"# INTRODUCTION"},{"metadata":{"_uuid":"febd4672ba0a0ae304561f30ec8a638fcec41da1"},"cell_type":"markdown","source":"**In this kernel,we will see K-Means and Hierarchical Clustering algorithms.**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b76dbe440ac9aadf8109760fe1e2f17fe5800459"},"cell_type":"markdown","source":"# K-Means Clustering"},{"metadata":{"_uuid":"1a389c0e007f02dde110f2c1812f514dea511e04"},"cell_type":"markdown","source":"* Firstly, let's start implementing these two algorithms in our own data set to understand better."},{"metadata":{"trusted":true,"_uuid":"6e2808cad11dc58efdfd61a067a9d2c278a27575"},"cell_type":"code","source":"#create dataset\n#np.random.normal(25,5,1000) =>\n#it's mean that 66%(666) of my data will be between 20(25-5) and 30(25+5)\n#This distribution is called Gaussian distribution.\n\n#for class1\nx1 = np.random.normal(25,5,1000)\ny1 = np.random.normal(25,5,1000)\n\n#for class2\nx2 = np.random.normal(55,5,1000)\ny2 = np.random.normal(60,5,1000)\n\n#for class3\nx3 = np.random.normal(55,5,1000)\ny3 = np.random.normal(15,5,1000)\n\nx = np.concatenate((x1,x2,x3), axis=0)\ny = np.concatenate((y1,y2,y3), axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cb8b1aa2aea3379ad48dc61deb75e52ec6e3d49b"},"cell_type":"code","source":"dictionary = {\"x\":x,\"y\":y}\ndata = pd.DataFrame(dictionary)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"033bd693bf01d78c5dc86d8cd1ebacb86ce175d9"},"cell_type":"code","source":"#my data look like this\n\nplt.scatter(x1,y1)\nplt.scatter(x2,y2)\nplt.scatter(x3,y3)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d6435bc3671f856c73b174ee69ce72e5982ac726"},"cell_type":"code","source":"#but kmeans algorithm will see this way\n\nplt.scatter(x1,y1,color=\"black\")\nplt.scatter(x2,y2,color=\"black\")\nplt.scatter(x3,y3,color=\"black\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"be6d5d28ab19e95841fe685f0717467e609564f4"},"cell_type":"code","source":"#K-MEANS\nfrom sklearn.cluster import KMeans\nwcss = []\n\nfor k in range(1,15):\n    kmeans = KMeans(n_clusters=k)\n    kmeans.fit(data)\n    #kmeans.inertia_ =>find wcss for each key value and add to list\n    wcss.append(kmeans.inertia_)\n\n#as we can see,optimal point is k=3\nplt.plot(range(1,15),wcss)\nplt.xlabel(\"number of k (cluster) value\")\nplt.ylabel(\"wcss\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9861205e1af026cf0f4f07e6e6d7696ea651deb8"},"cell_type":"markdown","source":"**According to Elbow Rule,we will choose 2 or 3.Because we don't know which one is better to use it.**"},{"metadata":{"trusted":true,"_uuid":"7b10d528ffaf0e222e94b480e9eca16c34c44a53"},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nplt.suptitle(\"K Means Clustering\",fontsize=20)\n\n#This is my model for k = 2.\n#fit_predict(data)=>it's mean that fit my data and create my clusters.\nkmeans2 = KMeans(n_clusters=1)\ncluster = kmeans2.fit_predict(data)\ndata[\"label\"] = cluster\nplt.subplot(1,3,1)\nplt.scatter(data.x[data.label == 0],data.y[data.label == 0],color=\"red\")\nplt.scatter(data.x[data.label == 1],data.y[data.label == 1],color=\"green\")\nplt.scatter(kmeans2.cluster_centers_[:,0],kmeans2.cluster_centers_[:,1],color=\"black\")\n\n# drop labels to use for k=2\ndata.drop([\"label\"],axis=1,inplace=True)\n\nkmeans2 = KMeans(n_clusters=2)\ncluster = kmeans2.fit_predict(data)\ndata[\"label\"] = cluster\nplt.subplot(1,3,2)\nplt.scatter(data.x[data.label == 0],data.y[data.label == 0],color=\"red\")\nplt.scatter(data.x[data.label == 1],data.y[data.label == 1],color=\"green\")\nplt.scatter(kmeans2.cluster_centers_[:,0],kmeans2.cluster_centers_[:,1],color=\"black\")\n\n# drop labels to use for k=3\ndata.drop([\"label\"],axis=1,inplace=True)\n\nkmeans2 = KMeans(n_clusters=3)\ncluster = kmeans2.fit_predict(data)\ndata[\"label\"] = cluster\nplt.subplot(1,3,3)\nplt.scatter(data.x[data.label == 0],data.y[data.label == 0],color=\"red\")\nplt.scatter(data.x[data.label == 1],data.y[data.label == 1],color=\"green\")\nplt.scatter(data.x[data.label == 2],data.y[data.label == 2],color=\"blue\")\nplt.scatter(kmeans2.cluster_centers_[:,0],kmeans2.cluster_centers_[:,1],color=\"black\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2643f8631f40e26835ec5f18e57417654e8d2a45"},"cell_type":"markdown","source":"**As we can see,we clearly seperate our classes using with K-Means Clustering Algorithm for k=3.**"},{"metadata":{"_uuid":"1bc71bd9f4c64c609768e3fb3a5ef8f7a992eccf"},"cell_type":"markdown","source":"# Hierarchical Clustering"},{"metadata":{"trusted":true,"_uuid":"8b39176e9e8d4d74f9d723aabccabec4751b38f5"},"cell_type":"code","source":"# dendogram\nfrom scipy.cluster.hierarchy import linkage, dendrogram\n\nmerg = linkage(data,method=\"ward\")\nplt.figure(figsize=(15,8))\ndendrogram(merg,leaf_rotation = 90)\nplt.xlabel(\"data points\")\nplt.ylabel(\"euclidean distance\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c869d820bee6293654c3d8d92d532045e981a332"},"cell_type":"markdown","source":"**We draw a line on the longest vertical line between horizontal lines.**<br><br>\n**Then we count how many vertical lines the line passes.**<br><br>\n**As you can see,it's look like k=3 but we will see.**"},{"metadata":{"trusted":true,"_uuid":"6c6ce1743a1ed88502b6aecac1a19f39cf63dc23"},"cell_type":"code","source":"# HC\nfrom sklearn.cluster import AgglomerativeClustering\n\nhiyerartical_cluster = AgglomerativeClustering(n_clusters = 3,affinity= \"euclidean\",linkage = \"ward\")\ncluster = hiyerartical_cluster.fit_predict(data)\n\ndata[\"label\"] = cluster\n\nplt.scatter(data.x[data.label == 0 ],data.y[data.label == 0],color = \"red\")\nplt.scatter(data.x[data.label == 1 ],data.y[data.label == 1],color = \"green\")\nplt.scatter(data.x[data.label == 2 ],data.y[data.label == 2],color = \"blue\")\nplt.scatter(kmeans2.cluster_centers_[:,0],kmeans2.cluster_centers_[:,1],color=\"black\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6b487c040f92166c352dd61e3e5c4ff41e458892"},"cell_type":"markdown","source":"## Let's try this algorithm on our Iris data."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/Iris.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5cf577d850f1feb620dec2647667c25bba27ed26"},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"337bc35c692dc83a1ed5c9ad98adddebfe91fe1b"},"cell_type":"code","source":"#we drop Id column because it's not a useful feature for us.\ndata.drop([\"Id\"],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c594dca740c3875c8a0b25b678bc63256bcc0ecd"},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b7e86df8463eab43f30bb2bba5b3c90123faf9c0"},"cell_type":"code","source":"sns.pairplot(data=data,hue=\"Species\",palette=\"Set1\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"108082ef624c6a571d7a324f4ecc2b567e7ff01b"},"cell_type":"code","source":"features = data.loc[:,[\"SepalLengthCm\",\"SepalWidthCm\",\"PetalLengthCm\",\"PetalWidthCm\"]]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aad3d1f0130667a3729e83c0ce86b30cffa0d74a"},"cell_type":"markdown","source":"# K-Means Clustering"},{"metadata":{"trusted":true,"_uuid":"7200c248c5b5021acdf4de911ae32c810292a3d2"},"cell_type":"code","source":"#K-MEANS\nfrom sklearn.cluster import KMeans\nwcss = []\n\nfor k in range(1,15):\n    kmeans = KMeans(n_clusters=k)\n    kmeans.fit(features)\n    #kmeans.inertia_ =>find wcss for each key value and add to list\n    wcss.append(kmeans.inertia_)\n\n#as we can see,optimal point is k=3\nplt.figure(figsize=(10,8))\nplt.plot(range(1,15),wcss,\"-o\")#\"-o\"=> for marker(point)\nplt.title(\"WCSS-K Chart\", fontsize=18)\nplt.grid(True)\nplt.xlabel(\"Number of K (cluster) Value\")\nplt.ylabel(\"WCSS\")\nplt.xticks(range(1,15))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"16292c6679c96f9035a4ee28e0d8ea4ea6717612"},"cell_type":"markdown","source":"**According to Elbow Rule,we will choose 2 or 3.Because we don't know which one is better to use it.**"},{"metadata":{"trusted":true,"_uuid":"d81a5e008f7fb9260c8c573fe729ac0b1b781e73"},"cell_type":"code","source":"plt.figure(figsize=(20,20))\nplt.suptitle(\"K Means Clustering\",fontsize=20)\n\n\nplt.subplot(3,2,1)\nplt.title(\"K = 1\",fontsize=16)\nplt.xlabel(\"PetalLengthCm\")\nplt.ylabel(\"PetalWidthCm\")\nplt.scatter(features.PetalLengthCm,features.PetalWidthCm)\n\n\nplt.subplot(3,2,2)\nplt.title(\"K = 2\",fontsize=16)\nplt.xlabel(\"PetalLengthCm\")\nkmeans = KMeans(n_clusters=2)\nfeatures[\"labels\"] = kmeans.fit_predict(features)\nplt.scatter(features.PetalLengthCm[features.labels == 0],features.PetalWidthCm[features.labels == 0])\nplt.scatter(features.PetalLengthCm[features.labels == 1],features.PetalWidthCm[features.labels == 1])\n\n# drop labels to use for k=3\nfeatures.drop([\"labels\"],axis=1,inplace=True)\n\nplt.subplot(3,2,3)\nplt.title(\"K = 3\",fontsize=16)\nplt.xlabel(\"PetalLengthCm\")\nkmeans = KMeans(n_clusters=3)\nfeatures[\"labels\"] = kmeans.fit_predict(features)\nplt.scatter(features.PetalLengthCm[features.labels == 0],features.PetalWidthCm[features.labels == 0])\nplt.scatter(features.PetalLengthCm[features.labels == 1],features.PetalWidthCm[features.labels == 1])\nplt.scatter(features.PetalLengthCm[features.labels == 2],features.PetalWidthCm[features.labels == 2])\n\n# drop labels to use for k=4\nfeatures.drop([\"labels\"],axis=1,inplace=True)\n\nplt.subplot(3,2,4)\nplt.title(\"K = 4\",fontsize=16)\nplt.xlabel(\"PetalLengthCm\")\nkmeans = KMeans(n_clusters=4)\nfeatures[\"labels\"] = kmeans.fit_predict(features)\nplt.scatter(features.PetalLengthCm[features.labels == 0],features.PetalWidthCm[features.labels == 0])\nplt.scatter(features.PetalLengthCm[features.labels == 1],features.PetalWidthCm[features.labels == 1])\nplt.scatter(features.PetalLengthCm[features.labels == 2],features.PetalWidthCm[features.labels == 2])\nplt.scatter(features.PetalLengthCm[features.labels == 3],features.PetalWidthCm[features.labels == 3])\n\n# drop labels to use for k=5\nfeatures.drop([\"labels\"],axis=1,inplace=True)\n\nplt.subplot(3,2,5)\nplt.title(\"K = 5\",fontsize=16)\nplt.xlabel(\"PetalLengthCm\")\nkmeans = KMeans(n_clusters=5)\nfeatures[\"labels\"] = kmeans.fit_predict(features)\nplt.scatter(features.PetalLengthCm[features.labels == 0],features.PetalWidthCm[features.labels == 0])\nplt.scatter(features.PetalLengthCm[features.labels == 1],features.PetalWidthCm[features.labels == 1])\nplt.scatter(features.PetalLengthCm[features.labels == 2],features.PetalWidthCm[features.labels == 2])\nplt.scatter(features.PetalLengthCm[features.labels == 3],features.PetalWidthCm[features.labels == 3])\nplt.scatter(features.PetalLengthCm[features.labels == 4],features.PetalWidthCm[features.labels == 4])\n\n# drop labels\nfeatures.drop([\"labels\"],axis=1,inplace=True)\n\nplt.subplot(3,2,6)\nplt.title(\"Original Labels\",fontsize=16)\nplt.xlabel(\"PetalLengthCm\")\nplt.scatter(data.PetalLengthCm[data.Species == \"Iris-setosa\"],data.PetalWidthCm[data.Species == \"Iris-setosa\"])\nplt.scatter(data.PetalLengthCm[data.Species == \"Iris-versicolor\"],data.PetalWidthCm[data.Species == \"Iris-versicolor\"])\nplt.scatter(data.PetalLengthCm[data.Species == \"Iris-virginica\"],data.PetalWidthCm[data.Species == \"Iris-virginica\"])\n\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dac4c5752d3404ef30ecc9a7e93e7163a8108601"},"cell_type":"markdown","source":"**As we can see,we clearly seperate our classes using with K-Means Clustering Algorithm for k=3.**"},{"metadata":{"_uuid":"cec1689cc2bb26b92d26cac0a7e08db8557a83f0"},"cell_type":"markdown","source":"# Hierarchical Clustering"},{"metadata":{"trusted":true,"_uuid":"316d46a21c197c47162ba14346a963e96209899c"},"cell_type":"code","source":"from scipy.cluster.hierarchy import dendrogram, linkage\n\nmerge = linkage(features,method=\"ward\")\n\nplt.figure(figsize=(15,8))\ndendrogram(merge, leaf_rotation=90)\nplt.xlabel(\"data points\")\nplt.ylabel(\"euclidian distance\")\nplt.suptitle(\"DENDROGRAM\",fontsize=18)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"56354a8d06a394c8ece86775511747ffdf25cb6f"},"cell_type":"code","source":"# we draw a line on the longest vertical line between horizontal lines.\n# Then we count how many vertical lines the line passes.\n# As you can see,it's look like k=2 but we will see.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cc922489b82d913947f6ef4ce16680ccbe2c979e"},"cell_type":"code","source":"from sklearn.cluster import AgglomerativeClustering\nplt.figure(figsize=(20,20))\nplt.suptitle(\"Hierarchical Clustering\",fontsize=20)\n\n\nplt.subplot(3,2,1)\nplt.title(\"K = 1\",fontsize=16)\nplt.xlabel(\"PetalLengthCm\")\nplt.ylabel(\"PetalWidthCm\")\nplt.scatter(features.PetalLengthCm,features.PetalWidthCm)\n\n\nplt.subplot(3,2,2)\nplt.title(\"K = 2\",fontsize=16)\nplt.xlabel(\"PetalLengthCm\")\nhc_cluster = AgglomerativeClustering(n_clusters=2)\nfeatures[\"labels\"] = hc_cluster.fit_predict(features)\nplt.scatter(features.PetalLengthCm[features.labels == 0],features.PetalWidthCm[features.labels == 0])\nplt.scatter(features.PetalLengthCm[features.labels == 1],features.PetalWidthCm[features.labels == 1])\n\n# drop labels to use for k=3\nfeatures.drop([\"labels\"],axis=1,inplace=True)\n\nplt.subplot(3,2,3)\nplt.title(\"K = 3\",fontsize=16)\nplt.xlabel(\"PetalLengthCm\")\nhc_cluster = AgglomerativeClustering(n_clusters=3)\nfeatures[\"labels\"] = hc_cluster.fit_predict(features)\nplt.scatter(features.PetalLengthCm[features.labels == 0],features.PetalWidthCm[features.labels == 0])\nplt.scatter(features.PetalLengthCm[features.labels == 1],features.PetalWidthCm[features.labels == 1])\nplt.scatter(features.PetalLengthCm[features.labels == 2],features.PetalWidthCm[features.labels == 2])\n\n# drop labels to use for k=4\nfeatures.drop([\"labels\"],axis=1,inplace=True)\n\nplt.subplot(3,2,4)\nplt.title(\"K = 4\",fontsize=16)\nplt.xlabel(\"PetalLengthCm\")\nhc_cluster = AgglomerativeClustering(n_clusters=4)\nfeatures[\"labels\"] = hc_cluster.fit_predict(features)\nplt.scatter(features.PetalLengthCm[features.labels == 0],features.PetalWidthCm[features.labels == 0])\nplt.scatter(features.PetalLengthCm[features.labels == 1],features.PetalWidthCm[features.labels == 1])\nplt.scatter(features.PetalLengthCm[features.labels == 2],features.PetalWidthCm[features.labels == 2])\nplt.scatter(features.PetalLengthCm[features.labels == 3],features.PetalWidthCm[features.labels == 3])\n\n# drop labels to use for k=5\nfeatures.drop([\"labels\"],axis=1,inplace=True)\n\nplt.subplot(3,2,5)\nplt.title(\"K = 5\",fontsize=16)\nplt.xlabel(\"PetalLengthCm\")\nhc_cluster = AgglomerativeClustering(n_clusters=5)\nfeatures[\"labels\"] = hc_cluster.fit_predict(features)\nplt.scatter(features.PetalLengthCm[features.labels == 0],features.PetalWidthCm[features.labels == 0])\nplt.scatter(features.PetalLengthCm[features.labels == 1],features.PetalWidthCm[features.labels == 1])\nplt.scatter(features.PetalLengthCm[features.labels == 2],features.PetalWidthCm[features.labels == 2])\nplt.scatter(features.PetalLengthCm[features.labels == 3],features.PetalWidthCm[features.labels == 3])\nplt.scatter(features.PetalLengthCm[features.labels == 4],features.PetalWidthCm[features.labels == 4])\n\n# drop labels\nfeatures.drop([\"labels\"],axis=1,inplace=True)\n\nplt.subplot(3,2,6)\nplt.title(\"Original Labels\",fontsize=16)\nplt.xlabel(\"PetalLengthCm\")\nplt.scatter(data.PetalLengthCm[data.Species == \"Iris-setosa\"],data.PetalWidthCm[data.Species == \"Iris-setosa\"])\nplt.scatter(data.PetalLengthCm[data.Species == \"Iris-versicolor\"],data.PetalWidthCm[data.Species == \"Iris-versicolor\"])\nplt.scatter(data.PetalLengthCm[data.Species == \"Iris-virginica\"],data.PetalWidthCm[data.Species == \"Iris-virginica\"])\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e31acb2d2fa46b6d704efcc8323231a1f45971b9"},"cell_type":"markdown","source":"**As we can see,we clearly seperate our classes using with Hierarchical Clustering Algorithm for k=3.**"},{"metadata":{"_uuid":"5ef289edfb684f739ce71f593cb102a3bc0a6c34"},"cell_type":"markdown","source":"# Conclusion<br>\n**If you like it, Please upvote my kernel.**<br>\n**If you have any question, I will happy to hear it**"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}