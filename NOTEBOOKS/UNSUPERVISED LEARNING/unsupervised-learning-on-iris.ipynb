{"cells":[{"metadata":{"_uuid":"7305492a60ff668b6ec6c50a4f8705d79fcaf3f7"},"cell_type":"markdown","source":"# Hi, this notebook is all about learning and experimenting Unsupervised Learning (on famous Iris Dataset)\n## You'll find: \n\n1- <a href=\"#eda\">EDA</a>\n\n2-  <a href=\"#1\">Adjusting the Dataset for Unsupervised Learning</a>\n\n3-  <a href=\"#2\">Implemeting the K Means Clustering</a>\n\n4-  <a href=\"#3\">Finding the best amount of clusters to get most accurate results (KMeans)</a>\n\n5-  <a href=\"#4\">Implemeting the Hierarchical Clustering</a>\n\n6-  <a href=\"#5\">Finding the best amount of clusters to get most accurate results (Hierarchy)</a>\n\n7-  <a href=\"#6\">Evaluating the Results and Comparing them</a>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"392203e3909501b8e517bf8cb6c043e64b56b0fe"},"cell_type":"markdown","source":"<div id=\"eda\"></div>\n# **BASIC EDA**"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/Iris.csv\")   # reading the data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c2a528ba16b0e120bfdd6c41937469cc9d155f20"},"cell_type":"code","source":"df.head()    # first 5 rows","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"79f4ab495216becc1605dfe4ceaf9dbbf3551a52"},"cell_type":"markdown","source":"Id column is not a real feature of our flowers. I will drop it"},{"metadata":{"trusted":true,"_uuid":"d592551949d7496e4473e6063225c7efe3a2fb80"},"cell_type":"code","source":"df.drop([\"Id\"],axis=1,inplace=True)    # dropped\n\ndf.tail()   # last 5 rows","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"089f454d2710935c5e0e15623e1a53b19ae7d810"},"cell_type":"code","source":"df.info()   # all non-null and numeric [except the labels]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"917e084a0c14c9af3f20e940ebf02b4a5fffc85b"},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"07727ed398f521e694b9f3e661aecb58e00df043"},"cell_type":"code","source":"sns.pairplot(data=df,hue=\"Species\",palette=\"Set2\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c356dbdb249c274abec0a6ebdca282fb270bb990"},"cell_type":"markdown","source":"### We see that iris-setosa is easily separable from the other two. Especially when we can see in different colors for corresponding Labels like above.\n\n### But our mission was finding the Labels that we didn't knew at all, So Let's create a suitable scenario."},{"metadata":{"_uuid":"bb670d0d5dc7392cade1ad5bf0a00fbe7d002384"},"cell_type":"markdown","source":"<div id=\"1\"></div>\n# ** Adjusting the Dataset for Unsupervised Learning **\n\n### I will simply do not use labels column on my *\"new\"* Dataset"},{"metadata":{"trusted":true,"_uuid":"12e1845fc937277479fbf2567572ea5ec9f032f2"},"cell_type":"code","source":"features = df.loc[:,[\"SepalLengthCm\",\"SepalWidthCm\",\"PetalLengthCm\",\"PetalWidthCm\"]]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d636b1338437539090ef4e34094d8889c281e839"},"cell_type":"markdown","source":"### From now on, we don't know the real labels or amount of labels anymore (Shhh!)"},{"metadata":{"_uuid":"6bafcfa7c0546e96626a152db0aa4083bf51c702"},"cell_type":"markdown","source":"<div id=\"2\"></div>\n# ** Implemeting the K Means Clustering **\n\n### SciKit-Learn implementation is very simple, it only takes 2 lines of code and 1 parameter"},{"metadata":{"trusted":true,"_uuid":"2a8729873c6fd6cf03652b98a7e6d1e78c3eba02"},"cell_type":"code","source":"from sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters=99)        # 99 CLUSTERS ?????","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"43b3e87c0c79077af843b99df83ab552072edfc2"},"cell_type":"markdown","source":"### WHY 99 ????? Because I don't know the right amount of Labels. Don't worry, There is a solution for it.\n\n\n<div id=\"3\"></div>\n# **Finding the best amount of clusters to get most accurate results (KMeans) **\n\n### I will use ELBOW RULE, which is basically looking for a plot line that respectively has a slope nearest to 90 degrees compared to y axis and be smallest possible. (yes, looks like an elbow )"},{"metadata":{"trusted":true,"_uuid":"353179d5eb554676b9e8a18cc2c7c477082993ae","scrolled":false},"cell_type":"code","source":"from sklearn.cluster import KMeans\nwcss = []\n\nfor k in range(1,15):\n    kmeans = KMeans(n_clusters=k)\n    kmeans.fit(features)\n    wcss.append(kmeans.inertia_)\n\n\nplt.figure(figsize=(20,8))\nplt.title(\"WCSS / K Chart\", fontsize=18)\nplt.plot(range(1,15),wcss,\"-o\")\nplt.grid(True)\nplt.xlabel(\"Amount of Clusters\",fontsize=14)\nplt.ylabel(\"Inertia\",fontsize=14)\nplt.xticks(range(1,20))\nplt.tight_layout()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"41b9e84e6642ca07d2f1925dd6e3f5f7451598a9"},"cell_type":"markdown","source":"### **3 or 2** seems to be our ** Best ** value(s) for clusters. (By the ** Elbow Rule**)"},{"metadata":{"_uuid":"30d761edddb2beda31175e6300ac004dee0b21e3"},"cell_type":"markdown","source":"## Let's Double Check it "},{"metadata":{"trusted":true,"_uuid":"077da0f8ce81a668937bb6c09902ec401834fe46"},"cell_type":"code","source":"plt.figure(figsize=(24,4))\n\nplt.suptitle(\"K Means Clustering\",fontsize=20)\n\n\nplt.subplot(1,5,1)\nplt.title(\"K = 1\",fontsize=16)\nplt.xlabel(\"PetalLengthCm\")\nplt.ylabel(\"PetalWidthCm\")\nplt.scatter(features.PetalLengthCm,features.PetalWidthCm)\n\n\nplt.subplot(1,5,2)\nplt.title(\"K = 2\",fontsize=16)\nplt.xlabel(\"PetalLengthCm\")\nkmeans = KMeans(n_clusters=2)\nfeatures[\"labels\"] = kmeans.fit_predict(features)\nplt.scatter(features.PetalLengthCm[features.labels == 0],features.PetalWidthCm[features.labels == 0])\nplt.scatter(features.PetalLengthCm[features.labels == 1],features.PetalWidthCm[features.labels == 1])\n\n# I drop labels since we only want to use features.\nfeatures.drop([\"labels\"],axis=1,inplace=True)\n\nplt.subplot(1,5,4)\nplt.title(\"K = 3\",fontsize=16)\nplt.xlabel(\"PetalLengthCm\")\nkmeans = KMeans(n_clusters=3)\nfeatures[\"labels\"] = kmeans.fit_predict(features)\nplt.scatter(features.PetalLengthCm[features.labels == 0],features.PetalWidthCm[features.labels == 0])\nplt.scatter(features.PetalLengthCm[features.labels == 1],features.PetalWidthCm[features.labels == 1])\nplt.scatter(features.PetalLengthCm[features.labels == 2],features.PetalWidthCm[features.labels == 2])\n\n# I drop labels since we only want to use features.\nfeatures.drop([\"labels\"],axis=1,inplace=True)\n\nplt.subplot(1,5,3)\nplt.title(\"K = 4\",fontsize=16)\nplt.xlabel(\"PetalLengthCm\")\nkmeans = KMeans(n_clusters=4)\nfeatures[\"labels\"] = kmeans.fit_predict(features)\nplt.scatter(features.PetalLengthCm[features.labels == 0],features.PetalWidthCm[features.labels == 0])\nplt.scatter(features.PetalLengthCm[features.labels == 1],features.PetalWidthCm[features.labels == 1])\nplt.scatter(features.PetalLengthCm[features.labels == 2],features.PetalWidthCm[features.labels == 2])\nplt.scatter(features.PetalLengthCm[features.labels == 3],features.PetalWidthCm[features.labels == 3])\n\n# I drop labels since we only want to use features.\nfeatures.drop([\"labels\"],axis=1,inplace=True)\n\nplt.subplot(1,5,5)\nplt.title(\"Original Labels\",fontsize=16)\nplt.xlabel(\"PetalLengthCm\")\nplt.scatter(df.PetalLengthCm[df.Species == \"Iris-setosa\"],df.PetalWidthCm[df.Species == \"Iris-setosa\"])\nplt.scatter(df.PetalLengthCm[df.Species == \"Iris-versicolor\"],df.PetalWidthCm[df.Species == \"Iris-versicolor\"])\nplt.scatter(df.PetalLengthCm[df.Species == \"Iris-virginica\"],df.PetalWidthCm[df.Species == \"Iris-virginica\"])\n\nplt.subplots_adjust(top=0.8)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d1172fe4642d8eb4ebc588a728fcf98a3552e524"},"cell_type":"markdown","source":"### Kmeans visibly did an amazing job with **3** clusters. Except few data points, I can say prediction is identical to the original with labels. Which shows that our ELBOW chart was right.\n\n<div id=\"4\"></div>\n# ** Implemeting the Hierarchical Clustering**\n\n### Again, Super Simple with SciKit-Learn. "},{"metadata":{"trusted":true,"_uuid":"82c5fb690f9a01ed9539a378dd786a23d002b7d9"},"cell_type":"code","source":"from sklearn.cluster import AgglomerativeClustering\nhc_cluster = AgglomerativeClustering(n_clusters=99)    # 99 CLUSTERS ????? ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"36f27215c495257376961b70d44579de1068cc2c"},"cell_type":"markdown","source":"### AGAIN 99 ????? Yes, Because I don't know the right amount of Labels. Again, There is also solution for it.\n<div id=\"5\"></div>\n# **Finding the best amount of clusters to get most accurate results (Hierarchy)**\n\n### Longest Vertical line between Horizontal Lines."},{"metadata":{"trusted":true,"_uuid":"9cdd916f1df249aebc1c2af32db6fe3a06f75ff9"},"cell_type":"code","source":"from scipy.cluster.hierarchy import dendrogram, linkage\n\nmerg = linkage(features,method=\"ward\")\n\nplt.figure(figsize=(18,6))\ndendrogram(merg, leaf_rotation=90)\nplt.xlabel(\"data points\")\nplt.ylabel(\"euclidian distance\")\n\nplt.suptitle(\"DENDROGRAM\",fontsize=18)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f5ca29965db9e48df4976ae6055520bdbb16b067"},"cell_type":"markdown","source":"### we see that longest vertical line without any perpendecular matching lines (euclidian distances). If we draw a horizontal line between that values, we will have ** 2 or 3 ** interceptions which are representing ideal amount of labels.\n\n"},{"metadata":{"trusted":true,"_uuid":"63b3f5dfb873eabb1b5b812b5e1870a28cc9e44a"},"cell_type":"markdown","source":"## Double Check!"},{"metadata":{"trusted":true,"_uuid":"8b3b17aac844624253c2b9ff47576c8fdff45b06"},"cell_type":"code","source":"plt.figure(figsize=(24,4))\n\nplt.suptitle(\"Hierarchical Clustering\",fontsize=20)\n\nplt.subplot(1,5,1)\nplt.title(\"K = 1\",fontsize=16)\nplt.xlabel(\"PetalLengthCm\")\nplt.ylabel(\"PetalWidthCm\")\nplt.scatter(features.SepalLengthCm,features.SepalWidthCm)\n\n\nplt.subplot(1,5,2)\nplt.title(\"K = 2\",fontsize=16)\nplt.xlabel(\"PetalLengthCm\")\nhc_cluster = AgglomerativeClustering(n_clusters=2)\nfeatures[\"labels\"] = hc_cluster.fit_predict(features)\nplt.scatter(features.SepalLengthCm[features.labels == 0],features.SepalWidthCm[features.labels == 0])\nplt.scatter(features.SepalLengthCm[features.labels == 1],features.SepalWidthCm[features.labels == 1])\n\n# I drop labels since we only want to use features.\nfeatures.drop([\"labels\"],axis=1,inplace=True)\n\nplt.subplot(1,5,3)\nplt.title(\"K = 4\",fontsize=16)\nplt.xlabel(\"PetalLengthCm\")\nhc_cluster = AgglomerativeClustering(n_clusters=4)\nfeatures[\"labels\"] = hc_cluster.fit_predict(features)\nplt.scatter(features.SepalLengthCm[features.labels == 0],features.SepalWidthCm[features.labels == 0])\nplt.scatter(features.SepalLengthCm[features.labels == 1],features.SepalWidthCm[features.labels == 1])\nplt.scatter(features.SepalLengthCm[features.labels == 2],features.SepalWidthCm[features.labels == 2])\nplt.scatter(features.SepalLengthCm[features.labels == 3],features.SepalWidthCm[features.labels == 3])\n\n# I drop labels since we only want to use features.\nfeatures.drop([\"labels\"],axis=1,inplace=True)\n\nplt.subplot(1,5,4)\nplt.title(\"K = 3\",fontsize=16)\nplt.xlabel(\"PetalLengthCm\")\nhc_cluster = AgglomerativeClustering(n_clusters=3)\nfeatures[\"labels\"] = hc_cluster.fit_predict(features)\nplt.scatter(features.SepalLengthCm[features.labels == 0],features.SepalWidthCm[features.labels == 0])\nplt.scatter(features.SepalLengthCm[features.labels == 1],features.SepalWidthCm[features.labels == 1])\nplt.scatter(features.SepalLengthCm[features.labels == 2],features.SepalWidthCm[features.labels == 2])\n\n\nplt.subplot(1,5,5)\nplt.title(\"Original Labels\",fontsize=16)\nplt.xlabel(\"PetalLengthCm\")\nplt.scatter(df.SepalLengthCm[df.Species == \"Iris-setosa\"],df.SepalWidthCm[df.Species == \"Iris-setosa\"])\nplt.scatter(df.SepalLengthCm[df.Species == \"Iris-versicolor\"],df.SepalWidthCm[df.Species == \"Iris-versicolor\"])\nplt.scatter(df.SepalLengthCm[df.Species == \"Iris-virginica\"],df.SepalWidthCm[df.Species == \"Iris-virginica\"])\n\nplt.subplots_adjust(top=0.8)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"035428699653c6faa108efc53fe0d90bc515444a"},"cell_type":"markdown","source":"### Again, Our double checking method showed that **3** is more accurate than ** 2 ** value by simply looking to the graph above. \n\n### Reason behind this is basically \"iris-setosa\" being too easy to separate while the other two is quite mixed and it made our Dendrogram method a bit unclear."},{"metadata":{"trusted":true,"_uuid":"d33441d87d33b3867448fa4e32ba1dfd6bfdd6f8"},"cell_type":"markdown","source":"<div id=\"6\"></div>\n# ** Evaluating the Results and Comparing them**"},{"metadata":{"trusted":true,"_uuid":"4826b560dfc8a1ac45a121fcc08db40978c39b35","scrolled":false},"cell_type":"code","source":"# I drop labels since we only want to use features.\nfeatures.drop([\"labels\"],axis=1,inplace=True)\n\n# kmeans\nkmeans = KMeans(n_clusters=3)\nkmeans_predict = kmeans.fit_predict(features)\n\n# cross tabulation table for kmeans\ndf1 = pd.DataFrame({'labels':kmeans_predict,\"Species\":df['Species']})\nct1 = pd.crosstab(df1['labels'],df1['Species'])\n\n\n# hierarchy\nhc_cluster = AgglomerativeClustering(n_clusters=3)\nhc_predict = hc_cluster.fit_predict(features)\n\n# cross tabulation table for Hierarchy\ndf2 = pd.DataFrame({'labels':hc_predict,\"Species\":df['Species']})\nct2 = pd.crosstab(df2['labels'],df2['Species'])\n\n\nplt.figure(figsize=(24,8))\nplt.suptitle(\"CROSS TABULATIONS\",fontsize=18)\nplt.subplot(1,2,1)\nplt.title(\"KMeans\")\nsns.heatmap(ct1,annot=True,cbar=False,cmap=\"Blues\")\n\nplt.subplot(1,2,2)\nplt.title(\"Hierarchy\")\nsns.heatmap(ct2,annot=True,cbar=False,cmap=\"Blues\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0f8d36d18a675a10f7d7e51f1ef44a64a097a1d6"},"cell_type":"markdown","source":"# **Conclusion**\n\n### The both Failed on 16 data points over 150 data points, which is equal to 90%\n\n### We also see that clustering \"iris-setosa\" was easy for both of them (50/50 success) because it's data points are all easily differentiable\n\n### 15 mistakes of all 16 is coming from \"iris-virginica\". Which shows that it was hard to cluster for my models.\n\n### ***Thanks for reading this far. If you enjoyed please be sure to vote, or if you have some issues don't be shy to comment. Best Regards. Efe***"},{"metadata":{"trusted":true,"_uuid":"eca081c9c37732f6c23b779ad9e0960039079a4d"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"abd7ea0d61d93394d86f32e592c8ef49a6224816"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"94dedf47696f325b65d3333e0ef6b9c7f451286f"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3bac1d6b31d7bbe6256f3db96f93fef6f314f9a8"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}